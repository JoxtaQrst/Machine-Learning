{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T22:34:18.150246800Z",
     "start_time": "2023-11-14T22:34:18.143576900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using real data from 2023 League of Legends World Championship\n",
    "# 2023_LoL_esports_match_data_from_OraclesElixir_20210622.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T22:34:18.166649700Z",
     "start_time": "2023-11-14T22:34:18.148149Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# 1. I create a script called \"dataset.py\" to preprocess the data and get only the information that i needed.\n",
    "\n",
    "# For example purposes, the csv file has the following columns:\n",
    "# gameid,datacompleteness,url,league,year,split,playoffs,date,game,patch,participantid,side,position,playername,playerid,teamname,teamid,champion,ban1,ban2,ban3,ban4,ban5,gamelength,result,kills,deaths,assists,teamkills,teamdeaths,doublekills,triplekills,quadrakills,pentakills,firstblood,firstbloodkill,firstbloodassist,firstbloodvictim,team kpm,ckpm,firstdragon,dragons,opp_dragons,elementaldrakes,opp_elementaldrakes,infernals,mountains,clouds,oceans,chemtechs,hextechs,dragons (type unknown),elders,opp_elders,firstherald,heralds,opp_heralds,firstbaron,barons,opp_barons,firsttower,towers,opp_towers,firstmidtower,firsttothreetowers,turretplates,opp_turretplates,inhibitors,opp_inhibitors,damagetochampions,dpm,damageshare,damagetakenperminute,damagemitigatedperminute,wardsplaced,wpm,wardskilled,wcpm,controlwardsbought,visionscore,vspm,totalgold,earnedgold,earned gpm,earnedgoldshare,goldspent,gspd,total cs,minionkills,monsterkills,monsterkillsownjungle,monsterkillsenemyjungle,cspm,goldat10,xpat10,csat10,opp_goldat10,opp_xpat10,opp_csat10,golddiffat10,xpdiffat10,csdiffat10,killsat10,assistsat10,deathsat10,opp_killsat10,opp_assistsat10,opp_deathsat10,goldat15,xpat15,csat15,opp_goldat15,opp_xpat15,opp_csat15,golddiffat15,xpdiffat15,csdiffat15,killsat15,assistsat15,deathsat15,opp_killsat15,opp_assistsat15,opp_deathsat15\n",
    "\n",
    "# Using the script, i chose the following columns:\n",
    "# 'champion', 'league', 'result', 'kills', 'deaths', 'assists','side', 'damagetochampions','position'\n",
    "# I split the \"position\" collumn into individual collumns and added 2 new collumns called \" Difficulty\" and \"Items_needed_for_spike\" to help me with the analysis. The choices made were from another csv file that had all the champions and their respective difficulty and items needed for spike called \"fabian.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T22:34:18.159130Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           result       kills      deaths     assists  damagetochampions\n",
      "count  300.000000  300.000000  300.000000  300.000000         300.000000\n",
      "mean     0.506667    2.796667    2.420000    6.173333       14555.590000\n",
      "std      0.500791    2.606011    1.800297    4.613122        9107.012622\n",
      "min      0.000000    0.000000    0.000000    0.000000        1327.000000\n",
      "25%      0.000000    1.000000    1.000000    3.000000        7832.250000\n",
      "50%      1.000000    2.000000    2.000000    5.000000       12564.500000\n",
      "75%      1.000000    4.000000    4.000000    8.250000       19273.500000\n",
      "max      1.000000   12.000000    9.000000   23.000000       50928.000000\n",
      "{'kills': [2.796666666666667, 6.79129319955407], 'deaths': [2.42, 3.241070234113713], 'assists': [6.173333333333333, 21.28089186176143], 'damagetochampions': [14555.59, 82937678.89153846]}\n",
      "Probabilities :  {'LCK': 0.25, 'LCS': 0.25, 'LEC': 0.25, 'LPL': 0.25}\n",
      "Entropy :  2.0\n",
      "Condititonal Entropy :  0.6205651394665301\n",
      "Information Gain :  0.005710049607652756\n",
      "Root Node: champion Information Gain: 0.15795704754326645\n",
      "Accuracy ID3 with most_common Handling: 0.5333333333333333\n",
      "Scikit-learn Accuracy on Test Data: 0.7444444444444445\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.5, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.5, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.5, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.5, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.5, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.5, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.5, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.5, 9.0, 9.0, 9.0, 9.0, 9.0, 9.5, 10.0, 10.0, 10.0, 10.5, 11.0, 11.5]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.svm._libsvm import predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## 1. Preprocessing\n",
    "\n",
    "# Read in data\n",
    "data = pd.read_csv('champions_info.csv')\n",
    "\n",
    "# b. Drop unnecessary columns\n",
    "data = data.dropna()\n",
    "\n",
    "# Describe the dataset\n",
    "print(data.describe())\n",
    "\n",
    "# a. Identify attributes and target attribute\n",
    "attributes = data.columns[:-1]  # all columns except the last one\n",
    "target_attribute = 'first_blood_kill'\n",
    "\n",
    "# a. Identify discrete and continuous attributes\n",
    "discrete_attributes = ['champion', 'league', 'result', 'side', 'Difficulty', 'Items_For_Spike', 'Attack_Type', 'top',\n",
    "                       'mid', 'jng', 'sup', 'bot']\n",
    "continuous_attributes = ['kills', 'deaths', 'assists', 'damagetochampions']\n",
    "\n",
    "# c. Calculate the mean and variance for each numerical attribute\n",
    "mean_variances = {}\n",
    "for attribute in continuous_attributes:\n",
    "    mean_variances[attribute] = [data[attribute].mean(), data[attribute].var()]\n",
    "\n",
    "print(mean_variances)\n",
    "\n",
    "\n",
    "## 3. Probabilities and Information Theory\n",
    "\n",
    "# a. Calculate the probability mass function of discrete attributes.\n",
    "\n",
    "def compute_probabilities(data, attribute):\n",
    "    return data[attribute].value_counts(normalize=True).to_dict()\n",
    "\n",
    "\n",
    "# e.g. compute_probabilities(data, 'league')\n",
    "probabilites = compute_probabilities(data, 'league')\n",
    "print(\"Probabilities : \", probabilites)\n",
    "\n",
    "\n",
    "# b. Calculate the entropy for discrete attributes.\n",
    "\n",
    "def calculate_entropy(data, attribute):\n",
    "    probabilities = compute_probabilities(data, attribute)\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities.values())\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# e.g. calculate_entropy(data, 'league')\n",
    "entropy_league = calculate_entropy(data, 'league')\n",
    "print(\"Entropy : \", entropy_league)\n",
    "\n",
    "\n",
    "# c. Calculate conditional entropy for target attribute and a discrete attribute.\n",
    "\n",
    "def calculate_conditional_entropy(data, attribute, target_attribute):\n",
    "    conditional_entropy = 0\n",
    "    for attribute_values in data[attribute].unique():\n",
    "        subset = data[data[attribute] == attribute_values]\n",
    "        probability_attribute_values = len(subset) / len(data)\n",
    "        entropy_attribute_value = calculate_entropy(subset, target_attribute)\n",
    "        conditional_entropy += probability_attribute_values * entropy_attribute_value\n",
    "    return conditional_entropy\n",
    "\n",
    "\n",
    "# e.g. calculate_conditional_entropy(data, 'league', 'first_blood_kill')\n",
    "conditional_entropy = calculate_conditional_entropy(data, 'league', 'first_blood_kill')\n",
    "print(\"Condititonal Entropy : \", conditional_entropy)\n",
    "\n",
    "\n",
    "# d. Calculate the infroamtion gain for discrete attributes.\n",
    "\n",
    "def calculate_information_gain(data, attribute, target_attribute):\n",
    "    target_entropy = calculate_entropy(data, target_attribute)\n",
    "    conditional_entropy = calculate_conditional_entropy(data, attribute, target_attribute)\n",
    "    information_gain = target_entropy - conditional_entropy\n",
    "    return information_gain\n",
    "\n",
    "\n",
    "# e.g. calculate_information_gain(data, 'league', 'first_blood_kill')\n",
    "information_gain = calculate_information_gain(data, 'league', 'first_blood_kill')\n",
    "print(\"Information Gain : \", information_gain)\n",
    "\n",
    "\n",
    "## 3. ID3\n",
    "\n",
    "# a. Find the root node of the decision tree.\n",
    "\n",
    "def find_root_node(data, attributes, target_attribute):\n",
    "    information_gains = [(attribute, calculate_information_gain(data, attribute, target_attribute)) for attribute in\n",
    "                         attributes]\n",
    "    best_attribute, best_information_gain = max(information_gains, key=lambda x: x[1])\n",
    "    return best_attribute, best_information_gain\n",
    "\n",
    "\n",
    "# e.g. find_root_node(data, attributes, target_attribute)\n",
    "root_node, root_ig = find_root_node(data, discrete_attributes, 'first_blood_kill')\n",
    "print(\"Root Node:\", root_node, \"Information Gain:\", root_ig)\n",
    "\n",
    "\n",
    "# b. . Write a function id3_discrete that implements the ID3 algorithm for the discrete attributes. The function should return a dictionary following this\n",
    "# structure\n",
    "def id3_discrete(data, attributes, target_attribute, unknown_handling=\"most_common\"):\n",
    "    if len(attributes) == 0:\n",
    "        most_common_value = data[target_attribute].mode().iloc[0]\n",
    "        return most_common_value\n",
    "\n",
    "    root_node, root_ig = find_root_node(data, attributes, target_attribute)\n",
    "    tree = {\n",
    "        \"node_attribute\": root_node,\n",
    "        \"observations\": dict(data[root_node].value_counts()),\n",
    "        \"information_gain\": root_ig,\n",
    "        \"values\": {}\n",
    "    }\n",
    "\n",
    "    unique_values = data[root_node].unique()\n",
    "    for value in unique_values:\n",
    "        subset = data[data[root_node] == value]\n",
    "        if len(subset[target_attribute].unique()) == 1:\n",
    "            tree[\"values\"][value] = subset[target_attribute].values[0]\n",
    "        else:\n",
    "            subtree = id3_discrete(subset.drop(columns=[root_node]), [attr for attr in attributes if attr != root_node],\n",
    "                                   target_attribute, unknown_handling)\n",
    "            tree[\"values\"][value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict_id3(tree, sample, unknown_handling=\"most_common\"):\n",
    "    current_node = tree\n",
    "    while isinstance(current_node, dict):\n",
    "        attribute_value = sample[current_node[\"node_attribute\"]]\n",
    "        if attribute_value in current_node[\"values\"]:\n",
    "            current_node = current_node[\"values\"][attribute_value]\n",
    "        else:\n",
    "            if unknown_handling == \"most_common\":\n",
    "                most_common_class = max(current_node[\"observations\"], key=current_node[\"observations\"].get)\n",
    "                return most_common_class\n",
    "            elif unknown_handling == \"random\":\n",
    "                return \"unknown_random\"\n",
    "            else:\n",
    "                return \"unknown_custom\"\n",
    "\n",
    "    return current_node\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tree = id3_discrete(data, discrete_attributes, 'first_blood_kill')\n",
    "\n",
    "\n",
    "# pprint(tree, width=40)\n",
    "\n",
    "# c. Run id3_discrete on the dataset containing only discrete attributes. Compare the results with the ones from sklearn . (make your comparison as\n",
    "# thorough as possible)\n",
    "\n",
    "def run_id3(data, discrete_attributes, target_attribute, unknown_handling=\"most_common\"):\n",
    "    # Split data into training and test set\n",
    "    unique_champions = data['champion'].unique()\n",
    "    train, test = train_test_split(data[data['champion'].isin(unique_champions)], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train ID3 tree with handling unknown values using the most common strategy\n",
    "    tree_id3_most_common = id3_discrete(train, discrete_attributes, target_attribute, unknown_handling=unknown_handling)\n",
    "\n",
    "    # Predict using ID3 with handling unknown values using the most common strategy\n",
    "    predictions_id3_test_most_common = [predict_id3(tree_id3_most_common, sample, unknown_handling=unknown_handling) for\n",
    "                                        _, sample in test.iterrows()]\n",
    "\n",
    "    # Convert the list to a pandas Series\n",
    "    predictions_id3_test_most_common_series = pd.Series(predictions_id3_test_most_common)\n",
    "\n",
    "    # Convert the data type of predictions_id3_test_most_common_series to match the data type of test[target_attribute]\n",
    "    predictions_id3_test_most_common_series = predictions_id3_test_most_common_series.astype(\n",
    "        test[target_attribute].dtype)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_id3_test_most_common = accuracy_score(test[target_attribute], predictions_id3_test_most_common_series)\n",
    "    print(f\"Accuracy ID3 with {unknown_handling} Handling: {accuracy_id3_test_most_common}\")\n",
    "\n",
    "    return accuracy_id3_test_most_common\n",
    "\n",
    "\n",
    "def run_sklearn_decision_tree(data, discrete_attributes, target_attribute):\n",
    "    # One-hot encode categorical features\n",
    "    X = data[discrete_attributes]\n",
    "    Y = data[target_attribute]\n",
    "    X_encoded = pd.get_dummies(X, columns=['champion', 'league', 'side', 'Difficulty', 'Items_For_Spike', 'Attack_Type',\n",
    "                                           'top', 'mid', 'jng', 'sup', 'bot'])\n",
    "    X_encoded[target_attribute] = Y\n",
    "\n",
    "    # Split data into training and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_encoded.drop(columns=[target_attribute]),\n",
    "                                                        X_encoded[target_attribute], test_size=0.3, random_state=42)\n",
    "\n",
    "    # Run scikit-learn Decision Tree\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    # Make Predictions\n",
    "    predictions_sklearn_test = clf.predict(X_test)\n",
    "\n",
    "    accuracy_sklearn_test = accuracy_score(Y_test, predictions_sklearn_test)\n",
    "\n",
    "    print(f\"Scikit-learn Accuracy on Test Data: {accuracy_sklearn_test}\")\n",
    "\n",
    "    return accuracy_sklearn_test\n",
    "\n",
    "\n",
    "run_id3(data, discrete_attributes, target_attribute)\n",
    "run_sklearn_decision_tree(data, discrete_attributes, target_attribute)\n",
    "\n",
    "\n",
    "# d. Write a function get_splits which, given a continuous attribute and the labels, will identify the splits that could be used to discretization of the\n",
    "# variable. Test your function on an example.\n",
    "\n",
    "def get_splits(labels, attribute_values):\n",
    "    # Combine attribute values and labels into a DataFrame for easier sorting\n",
    "    data = pd.DataFrame({'labels': labels, 'values': attribute_values})\n",
    "\n",
    "    # Sort data by values\n",
    "    sorted_data = data.sort_values(by=['values'])\n",
    "\n",
    "    # Identify candidate split points as the midpoints between consecutive values\n",
    "    splits = [(sorted_data['values'].iloc[i] + sorted_data['values'].iloc[i + 1]) / 2 for i in\n",
    "              range(len(sorted_data['values']) - 1)]\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "# Example usage using my data\n",
    "splits = get_splits(data['first_blood_kill'], data['kills'])\n",
    "print(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
